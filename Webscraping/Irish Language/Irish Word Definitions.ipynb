{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python script for getting the definitions of 53,999 Irish words inorder to make flash cards out of them. This code is intended to show\n",
    "how this was done, rather than intended for replication.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests\n",
    "from string import ascii_lowercase\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root url\n",
    "root = \"https://www.teanglann.ie/en/fgb/_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for all the original letters\n",
    "alphabet = []\n",
    "for c in ascii_lowercase:\n",
    "    alphabet.append(root+c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as each letter has its own page, a list of all 26 letters must be cycled through to obtain all the words\n",
    "soup_kitchen = []\n",
    "for element in range(0,26): soup_kitchen.append(BeautifulSoup(requests.get(alphabet[element]).text,'html.parser').find_all(\"span\",{\"class\": \"abcItem\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise the list of links and fill it with the word definitions\n",
    "all_links = []\n",
    "for i in range(0,26):\n",
    "    for element in soup_kitchen[i]:\n",
    "        all_links.append(element.find_all('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the word from each link\n",
    "all_text = []\n",
    "for link in all_links:\n",
    "   all_text.append(link[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new root for definitions pages\n",
    "root = \"https://www.teanglann.ie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of all definition urls\n",
    "full_urls = []\n",
    "for link in all_links:\n",
    "    for bit in link:\n",
    "        full_urls.append(root+list(bit.attrs.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through all urls and scrape each url's word, grammatical features and first definition\n",
    "final_results = []\n",
    "for link in full_urls:\n",
    "    data = requests.get(link)\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    titles = []\n",
    "    grammars = []\n",
    "    definitions = []\n",
    "    for index, entry in enumerate(soup.select(\"body div#envelope div#invelope .listings .fgb.entry .fgb.title\")):\n",
    "        if index == 0: #flashcards cannot be too long, so cut off at first definition\n",
    "            titles.append(entry.text+' ('+str(index+1)+')')\n",
    "        for grammar in entry.parent.select(\".fgb.g\"):\n",
    "            grammars.append(grammar.text)\n",
    "        for definition in entry.parent.select(\".fgb.trans\"):\n",
    "            definitions.append(definition.text)\n",
    "    \n",
    "    result = zip(titles,grammars,definitions)\n",
    "    result_set = set(result)\n",
    "    final_results.append(sorted(result_set)) \n",
    "    time.sleep(random.randint(1,5)*random.random()) #delay for politeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a dataframe of the results\n",
    "definitions = pd.DataFrame(final_results,columns = ['definition'])\n",
    "definitions['word'] = all_text #original word column included to join definitions to audio file in instances where the first definition does not perfectly match the word text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty definitions\n",
    "defineds = definitions.loc[definitions['definition1'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save definitions as excel document\n",
    "defineds.to_excel('definitions.xlsx',sheet_name='Sheet1', na_rep='', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
